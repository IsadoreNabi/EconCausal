<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="José Mauricio Gómez Julián" />


<title>Methodological Details of the Bayesian GLM Model with AR(1) Structure</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>







<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore"><strong>Methodological Details of the
Bayesian GLM Model with AR(1) Structure</strong></h1>
<h4 class="author">José Mauricio Gómez Julián</h4>
<h4 class="date">octubre 2025</h4>


<div id="TOC">
<ul>
<li><a href="#bayesian-framework-for-time-series-modeling-with-autoregressive-structure" id="toc-bayesian-framework-for-time-series-modeling-with-autoregressive-structure"><strong>Bayesian
Framework for Time Series Modeling with Autoregressive
Structure</strong></a>
<ul>
<li><a href="#specification-of-the-bayesian-model-with-ar1" id="toc-specification-of-the-bayesian-model-with-ar1"><strong>Specification
of the Bayesian Model with AR(1)</strong></a>
<ul>
<li><a href="#complete-model-structure" id="toc-complete-model-structure"><strong>Complete Model
Structure</strong></a></li>
<li><a href="#reference-baseline-model" id="toc-reference-baseline-model"><strong>Reference Baseline
Model</strong></a></li>
<li><a href="#prior-specification" id="toc-prior-specification"><strong>Prior
Specification</strong></a></li>
</ul></li>
<li><a href="#temporal-cross-validation-with-leave-future-out-lfo" id="toc-temporal-cross-validation-with-leave-future-out-lfo"><strong>Temporal
Cross-Validation with Leave-Future-Out (LFO)</strong></a>
<ul>
<li><a href="#rolling-origin-implementation-with-sliding-window" id="toc-rolling-origin-implementation-with-sliding-window"><strong>Rolling-Origin
Implementation with Sliding Window</strong></a></li>
<li><a href="#predictive-evaluation-criteria" id="toc-predictive-evaluation-criteria"><strong>Predictive Evaluation
Criteria</strong></a></li>
<li><a href="#victory-criterion-per-fold" id="toc-victory-criterion-per-fold"><strong>Victory Criterion per
Fold</strong></a></li>
</ul></li>
<li><a href="#bayesian-inference-with-hamiltonian-monte-carlo" id="toc-bayesian-inference-with-hamiltonian-monte-carlo"><strong>Bayesian
Inference with Hamiltonian Monte Carlo</strong></a>
<ul>
<li><a href="#nuts-sampler-configuration" id="toc-nuts-sampler-configuration"><strong>NUTS Sampler
Configuration</strong></a></li>
<li><a href="#convergence-diagnostics" id="toc-convergence-diagnostics"><strong>Convergence
Diagnostics</strong></a></li>
</ul></li>
<li><a href="#determination-of-temporal-stability-and-support" id="toc-determination-of-temporal-stability-and-support"><strong>Determination
of Temporal Stability and Support</strong></a>
<ul>
<li><a href="#support-metric" id="toc-support-metric"><strong>Support
Metric</strong></a></li>
<li><a href="#robustness-thresholds" id="toc-robustness-thresholds"><strong>Robustness
Thresholds</strong></a></li>
<li><a href="#support-interpretation" id="toc-support-interpretation"><strong>Support
Interpretation</strong></a></li>
</ul></li>
<li><a href="#comparative-analysis-with-ecm-mars-methodology" id="toc-comparative-analysis-with-ecm-mars-methodology"><strong>Comparative
Analysis with ECM-MARS Methodology</strong></a>
<ul>
<li><a href="#fundamental-differences" id="toc-fundamental-differences"><strong>Fundamental
Differences</strong></a></li>
<li><a href="#advantages-of-the-bayesian-approach" id="toc-advantages-of-the-bayesian-approach"><strong>Advantages of the
Bayesian Approach</strong></a></li>
<li><a href="#relative-limitations" id="toc-relative-limitations"><strong>Relative
Limitations</strong></a></li>
</ul></li>
<li><a href="#technical-implementation-and-optimizations" id="toc-technical-implementation-and-optimizations"><strong>Technical
Implementation and Optimizations</strong></a>
<ul>
<li><a href="#memory-management-and-parallelization" id="toc-memory-management-and-parallelization"><strong>Memory Management
and Parallelization</strong></a></li>
<li><a href="#handling-degenerate-cases" id="toc-handling-degenerate-cases"><strong>Handling Degenerate
Cases</strong></a></li>
</ul></li>
<li><a href="#complete-pipeline-pseudocode" id="toc-complete-pipeline-pseudocode"><strong>Complete Pipeline
Pseudocode</strong></a></li>
<li><a href="#technical-notes-and-special-considerations" id="toc-technical-notes-and-special-considerations"><strong>Technical
Notes and Special Considerations</strong></a>
<ul>
<li><a href="#scaling-and-numerical-stability" id="toc-scaling-and-numerical-stability"><strong>Scaling and Numerical
Stability</strong></a></li>
<li><a href="#interpretation-of-negative-metrics" id="toc-interpretation-of-negative-metrics"><strong>Interpretation of
Negative Metrics</strong></a></li>
<li><a href="#potential-extensions" id="toc-potential-extensions"><strong>Potential
Extensions</strong></a></li>
</ul></li>
<li><a href="#methodological-conclusions" id="toc-methodological-conclusions"><strong>Methodological
Conclusions</strong></a></li>
</ul></li>
</ul>
</div>

<div id="bayesian-framework-for-time-series-modeling-with-autoregressive-structure" class="section level1">
<h1><strong>Bayesian Framework for Time Series Modeling with
Autoregressive Structure</strong></h1>
<p>This document details the methodology implemented for analyzing
causal relationships between production and circulation variables using
a Bayesian Generalized Linear Model (BGLM) with first-order
autoregressive structure AR(1). This approach differs substantially from
the ECM-MARS methodology by adopting a complete Bayesian inferential
paradigm, allowing comprehensive quantification of predictive
uncertainty and model comparison through predictive information
criteria.</p>
<div id="specification-of-the-bayesian-model-with-ar1" class="section level2">
<h2><strong>Specification of the Bayesian Model with AR(1)</strong></h2>
<div id="complete-model-structure" class="section level3">
<h3><strong>Complete Model Structure</strong></h3>
<p>Let <span class="math inline">\(Y_t\)</span> be the dependent
variable and <span class="math inline">\(X_t\)</span> the independent
variable at time <span class="math inline">\(t\)</span>. The complete
model is specified as:</p>
<p><span class="math display">\[Y_{t,s} = \alpha + \beta_0 \cdot t_s +
\sum_{i=1}^{L} \beta_i X_{t-i,s} + \epsilon_t\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(Y_{t,s}\)</span> is the standardized
dependent variable: <span class="math inline">\((Y_t -
\mu_Y)/\sigma_Y\)</span></li>
<li><span class="math inline">\(t_s\)</span> is the standardized
temporal index: <span class="math inline">\((t -
\mu_t)/\sigma_t\)</span></li>
<li><span class="math inline">\(X_{t-i,s}\)</span> are the standardized
lags of the independent variable</li>
<li><span class="math inline">\(L\)</span> is the maximum number of lags
(default <span class="math inline">\(L=3\)</span>)</li>
<li><span class="math inline">\(\epsilon_t\)</span> follows an AR(1)
process: <span class="math inline">\(\epsilon_t = \phi \epsilon_{t-1} +
\eta_t\)</span>, with <span class="math inline">\(\eta_t \sim N(0,
\sigma^2)\)</span></li>
</ul>
<p>Standardization is critical for the efficiency of the NUTS<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> (No-U-Turn
Sampler) algorithm, ensuring that all parameters operate on comparable
scales and avoiding convergence issues in the Hamiltonian sampler.</p>
</div>
<div id="reference-baseline-model" class="section level3">
<h3><strong>Reference Baseline Model</strong></h3>
<p>As a comparison benchmark, we estimate a baseline model that excludes
information from the independent variable:</p>
<p><span class="math display">\[Y_{t,s} = \alpha + \beta_0 \cdot t_s +
\epsilon_t\]</span></p>
<p>This model captures only the temporal trend and the intrinsic
autoregressive structure of <span class="math inline">\(Y\)</span>,
serving as an effective null hypothesis against which to evaluate the
predictive contribution of the lags of <span class="math inline">\(X\)</span>.</p>
</div>
<div id="prior-specification" class="section level3">
<h3><strong>Prior Specification</strong></h3>
<p>Priors are specified on the standardized scale to maintain
consistency:</p>
<ul>
<li><strong>Regression coefficients</strong> (<span class="math inline">\(\beta_i\)</span>): <span class="math inline">\(\beta_i \sim N(0, 1)\)</span></li>
<li><strong>Intercept</strong> (<span class="math inline">\(\alpha\)</span>): <span class="math inline">\(\alpha \sim t_3(0, 2.5)\)</span> (Student-t with 3
degrees of freedom)</li>
<li><strong>Residual standard deviation</strong> (<span class="math inline">\(\sigma\)</span>): <span class="math inline">\(\sigma \sim \text{Exponential}(1)\)</span></li>
<li><strong>AR(1) coefficient</strong> (<span class="math inline">\(\phi\)</span>): Implicit uniform prior on <span class="math inline">\((-1, 1)\)</span> for stationarity</li>
</ul>
<p>The choice of weakly informative priors<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> reflects a balance
between mild regularization and flexibility to learn from the data. The
Student-t distribution for the intercept provides robustness against
outliers, while the exponential prior for <span class="math inline">\(\sigma\)</span> ensures positivity with a heavy
tail.</p>
</div>
</div>
<div id="temporal-cross-validation-with-leave-future-out-lfo" class="section level2">
<h2><strong>Temporal Cross-Validation with Leave-Future-Out
(LFO)</strong></h2>
<div id="rolling-origin-implementation-with-sliding-window" class="section level3">
<h3><strong>Rolling-Origin Implementation with Sliding
Window</strong></h3>
<p>Unlike traditional cross-validation that violates the principle of
temporal causality, we implement Leave-Future-Out (LFO) with a sliding
window. This scheme strictly respects temporal ordering and simulates
the real context of prospective forecasting.</p>
<p>The procedure is structured as follows:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Initialization</strong>: The initial training set
comprises 70% of the data or a minimum of 90 observations, whichever is
greater.</p></li>
<li><p><strong>Test horizon</strong>: Each fold evaluates <span class="math inline">\(h = 12\)</span> months ahead, simulating annual
forecasts.</p></li>
<li><p><strong>Step between folds</strong>: The origin shifts 12 months
between successive evaluations.</p></li>
<li><p><strong>Sliding window</strong>: The training set size remains
constant<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>, discarding old observations when
incorporating new ones. This contrasts with the expanding window where
the training set grows monotonically.</p></li>
</ol>
<p>The advantage of the sliding window lies in its sensitivity to regime
changes and its ability to adapt to broadly non-stationary dynamics.
While an expanding window assumes constant parameters throughout the
entire history, the sliding window recognizes that economic
relationships may evolve, maintaining only the most relevant “recent
memory” for forecasting.</p>
</div>
<div id="predictive-evaluation-criteria" class="section level3">
<h3><strong>Predictive Evaluation Criteria</strong></h3>
<p>For each fold <span class="math inline">\(k\)</span>, we compare the
predictive performance of the complete model against the baseline model
using two complementary criteria:</p>
<div id="expected-log-predictive-density-elpd" class="section level4">
<h4><strong>Expected Log Predictive Density (ELPD)</strong></h4>
<p>The ELPD<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> quantifies the expected log predictive
density for new observations:</p>
<p><span class="math display">\[\text{ELPD}_k = \sum_{t \in
\text{test}_k} \log \int p(y_t | \theta) p(\theta |
\text{data}_{\text{train},k}) d\theta\]</span></p>
<p>In practice, we approximate this integral using the log-sum-exp
estimator over posterior samples:</p>
<p><span class="math display">\[\widehat{\text{ELPD}}_k = \sum_{t \in
\text{test}_k} \log \left( \frac{1}{S} \sum_{s=1}^S p(y_t |
\theta^{(s)}) \right)\]</span></p>
<p>where <span class="math inline">\(\theta^{(s)}\)</span> are the
posterior samples and <span class="math inline">\(S\)</span> is the
number of post-warmup iterations.</p>
<p>The difference <span class="math inline">\(\Delta\text{ELPD}_k =
\text{ELPD}_{k,\text{full}} - \text{ELPD}_{k,\text{base}}\)</span>
measures the gain in predictive capacity. A <span class="math inline">\(\Delta\text{ELPD} &gt; 0\)</span> indicates that
the complete model assigns higher probability to the observed
out-of-sample data.</p>
</div>
<div id="root-mean-square-error-rmse-and-classical-metrics" class="section level4">
<h4><strong>Root Mean Square Error (RMSE) and Classical
Metrics</strong></h4>
<p>We complement the ELPD with traditional metrics on the original scale
of the variables:</p>
<ul>
<li><strong>RMSE</strong>: <span class="math inline">\(\sqrt{\frac{1}{n_{\text{test}}} \sum_{t} (y_t -
\hat{y}_t)^2}\)</span></li>
<li><strong>MAE</strong>: <span class="math inline">\(\frac{1}{n_{\text{test}}} \sum_{t} |y_t -
\hat{y}_t|\)</span></li>
<li><strong>sMAPE</strong>: <span class="math inline">\(\frac{100}{n_{\text{test}}} \sum_{t} \frac{2|y_t -
\hat{y}_t|}{|y_t| + |\hat{y}_t|}\)</span></li>
<li><strong>R²</strong>: <span class="math inline">\(1 - \frac{\sum_t
(y_t - \hat{y}_t)^2}{\sum_t (y_t - \bar{y})^2}\)</span> (protected
against <span class="math inline">\(SST \approx 0\)</span>)</li>
</ul>
<p>where <span class="math inline">\(\hat{y}_t\)</span> is the posterior
mean of predictions, obtained via <code>posterior_epred()</code> and
transformed back to the original scale.</p>
</div>
</div>
<div id="victory-criterion-per-fold" class="section level3">
<h3><strong>Victory Criterion per Fold</strong></h3>
<p>A complete model “wins” in fold <span class="math inline">\(k\)</span> if and only if:</p>
<p><span class="math display">\[\begin{cases}
\Delta\text{ELPD}_k &gt; 0 &amp; \text{(better predictive density)} \\
\Delta\text{RMSE}_k &lt; 0 &amp; \text{(lower squared error)}
\end{cases}\]</span></p>
<p>This dual criterion requires superiority in both probabilistic (ELPD)
and deterministic (RMSE) terms, avoiding spurious victories from
improvements in a single dimension.</p>
</div>
</div>
<div id="bayesian-inference-with-hamiltonian-monte-carlo" class="section level2">
<h2><strong>Bayesian Inference with Hamiltonian Monte
Carlo</strong></h2>
<div id="nuts-sampler-configuration" class="section level3">
<h3><strong>NUTS Sampler Configuration</strong></h3>
<p>Inference is performed using the NUTS<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> algorithm implemented
in Stan through <code>brms</code> with <code>cmdstanr</code> backend.
Sampling parameters are calibrated to ensure robust convergence:</p>
<ul>
<li><strong>Chains</strong>: 4 independent chains</li>
<li><strong>Total iterations</strong>: 1500 per chain</li>
<li><strong>Warmup</strong>: 750 iterations (50% for adaptation)</li>
<li><strong>Adapt delta</strong>: 0.95 (reduces divergent steps)</li>
<li><strong>Max treedepth</strong>: 12 (avoids premature tree
truncation)</li>
</ul>
<p>Parallelization is implemented at the chain level
(<code>parallel_chains = 4</code>), not within each chain, to maintain
sampler efficiency without CPU oversubscription.</p>
</div>
<div id="convergence-diagnostics" class="section level3">
<h3><strong>Convergence Diagnostics</strong></h3>
<p>Although not explicitly reported in production code for efficiency,
standard diagnostics include:</p>
<ul>
<li><strong><span class="math inline">\(\hat{R}\)</span>
(Rhat)</strong>: Should be <span class="math inline">\(&lt;
1.01\)</span> for all parameters</li>
<li><strong>ESS (Effective Sample Size)</strong>: Bulk-ESS and Tail-ESS
<span class="math inline">\(&gt; 400\)</span> per chain</li>
<li><strong>Divergences</strong>: Ideally 0; few divergences (&lt;1%)
may be tolerable</li>
<li><strong>BFMI (Bayesian Fraction of Missing Information)</strong>:
<span class="math inline">\(&gt; 0.2\)</span> indicates good
behavior</li>
</ul>
</div>
</div>
<div id="determination-of-temporal-stability-and-support" class="section level2">
<h2><strong>Determination of Temporal Stability and
Support</strong></h2>
<div id="support-metric" class="section level3">
<h3><strong>Support Metric</strong></h3>
<p>We define support as:</p>
<p><span class="math display">\[\text{support} =
\frac{\text{folds\_pass}}{\text{folds}}\]</span></p>
<p>where <code>folds_pass</code> counts the folds where the complete
model satisfies the dual victory criterion. This metric quantifies the
temporal consistency of predictive power.</p>
</div>
<div id="robustness-thresholds" class="section level3">
<h3><strong>Robustness Thresholds</strong></h3>
<p>We establish two confidence levels:</p>
<ul>
<li><strong>Strict threshold</strong>: support <span class="math inline">\(\geq 0.70\)</span> with minimum 5 valid folds</li>
<li><strong>Moderate threshold</strong>: support <span class="math inline">\(\geq 0.60\)</span> with minimum 5 valid folds</li>
</ul>
<p>The requirement of an absolute minimum of folds prevents false
positives from small denominators (e.g., 1/1 = 100% support does not
constitute robust evidence).</p>
</div>
<div id="support-interpretation" class="section level3">
<h3><strong>Support Interpretation</strong></h3>
<p>High support indicates that the predictive relationship persists
across different temporal regimes. Support values:</p>
<ul>
<li><strong>0.80-1.00</strong>: Very stable relationship, robust to
regime changes</li>
<li><strong>0.60-0.79</strong>: Moderately stable relationship,
sensitive to some epochs</li>
<li><strong>0.40-0.59</strong>: Unstable relationship, dependent on
temporal context</li>
<li><strong>&lt; 0.40</strong>: Spurious relationship or highly specific
to particular periods</li>
</ul>
</div>
</div>
<div id="comparative-analysis-with-ecm-mars-methodology" class="section level2">
<h2><strong>Comparative Analysis with ECM-MARS Methodology</strong></h2>
<div id="fundamental-differences" class="section level3">
<h3><strong>Fundamental Differences</strong></h3>
<table>
<colgroup>
<col width="22%" />
<col width="25%" />
<col width="52%" />
</colgroup>
<thead>
<tr class="header">
<th>Aspect</th>
<th>ECM-MARS</th>
<th>Bayesian GLM AR(1)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Paradigm</strong></td>
<td>Frequentist with sequential tests</td>
<td>Integral Bayesian</td>
</tr>
<tr class="even">
<td><strong>Cointegration</strong></td>
<td>Explicit requirement (Engle-Granger/Johansen)</td>
<td>Implicit in AR(1) structure</td>
</tr>
<tr class="odd">
<td><strong>Non-linearity</strong></td>
<td>MARS with adaptive splines</td>
<td>Linear in parameters</td>
</tr>
<tr class="even">
<td><strong>Uncertainty</strong></td>
<td>HAC standard errors</td>
<td>Complete posterior distribution</td>
</tr>
<tr class="odd">
<td><strong>Comparison</strong></td>
<td>Significance tests</td>
<td>ELPD + predictive metrics</td>
</tr>
<tr class="even">
<td><strong>Complexity</strong></td>
<td>High (multiple pre-tests)</td>
<td>Moderate (direct model)</td>
</tr>
</tbody>
</table>
</div>
<div id="advantages-of-the-bayesian-approach" class="section level3">
<h3><strong>Advantages of the Bayesian Approach</strong></h3>
<ol style="list-style-type: decimal">
<li><p><strong>Comprehensive uncertainty quantification</strong>:
Posterior distributions capture all parametric uncertainty, naturally
propagating it to predictions.</p></li>
<li><p><strong>Direct model comparison</strong>: ELPD provides a unified
metric for comparing non-nested models without multiplicity
adjustments.</p></li>
<li><p><strong>Robustness to assumption violations</strong>:
Regularizing priors and AR(1) structure accommodate moderate deviations
from classical assumptions.</p></li>
<li><p><strong>Interpretability</strong>: Credible intervals have direct
probabilistic interpretation, unlike frequentist confidence
intervals.</p></li>
</ol>
</div>
<div id="relative-limitations" class="section level3">
<h3><strong>Relative Limitations</strong></h3>
<ol style="list-style-type: decimal">
<li><p><strong>Computational cost</strong>: MCMC sampling is orders of
magnitude slower than OLS/MARS estimation.</p></li>
<li><p><strong>Prior sensitivity</strong>: In small samples, prior
choice can substantially influence results.</p></li>
<li><p><strong>Linearity</strong>: The absence of non-linear terms
(splines) may limit flexibility to capture complex
relationships.</p></li>
</ol>
</div>
</div>
<div id="technical-implementation-and-optimizations" class="section level2">
<h2><strong>Technical Implementation and Optimizations</strong></h2>
<div id="memory-management-and-parallelization" class="section level3">
<h3><strong>Memory Management and Parallelization</strong></h3>
<ul>
<li><p><strong>Pairwise parallelization</strong>: Each pair <span class="math inline">\((X \to Y)\)</span> is processed sequentially, but
MCMC chains within each model are parallelized.</p></li>
<li><p><strong>Memory release</strong>: Stan objects are implicitly
released after each fold through R’s garbage collector.</p></li>
<li><p><strong>cmdstanr backend</strong>: More efficient than rstan for
large models, with better memory management and diagnostics.</p></li>
</ul>
</div>
<div id="handling-degenerate-cases" class="section level3">
<h3><strong>Handling Degenerate Cases</strong></h3>
<p>The code includes safeguards for:</p>
<ul>
<li><strong>Near-zero variance</strong>: If <span class="math inline">\(\sigma_Y \approx 0\)</span> in the training set,
the fold is skipped.</li>
<li><strong>Convergence errors</strong>: Models that fail to converge
return <code>NULL</code> and are excluded from analysis.</li>
<li><strong>Undefined log-likelihoods</strong>: Finite values are
verified before calculating ELPD.</li>
</ul>
</div>
</div>
<div id="complete-pipeline-pseudocode" class="section level2">
<h2><strong>Complete Pipeline Pseudocode</strong></h2>
<pre><code>FOR each pair (X, Y) in {production × circulation} × {2 directions}:
  1. Build matrix with Y, X, and lags(X, 1:L)
  2. Remove rows with missing values
  
  FOR each fold in rolling_splits(sliding_window):
    3. Split into train/test
    4. Standardize variables using train statistics
    5. Fit baseline_model: Y_s ~ 1 + t_s + AR(1)
    6. Fit full_model: Y_s ~ 1 + t_s + X_lags + AR(1)
    7. Calculate ELPD for both models on test
    8. Generate point predictions (posterior_epred)
    9. Transform predictions to original scale
    10. Compute metrics (RMSE, MAE, sMAPE, R²)
    11. Determine victory: (ΔELPD &gt; 0) ∧ (ΔRMSE &lt; 0)
  
  12. Calculate support = wins / total_folds
  13. Average metrics across folds

14. Ranking by (support DESC, ΔELPD DESC, ΔRMSE ASC)
15. Filter winners by support thresholds
16. Export results and visualizations</code></pre>
</div>
<div id="technical-notes-and-special-considerations" class="section level2">
<h2><strong>Technical Notes and Special Considerations</strong></h2>
<div id="scaling-and-numerical-stability" class="section level3">
<h3><strong>Scaling and Numerical Stability</strong></h3>
<p>Per-fold scaling (not global) is crucial because:</p>
<ol style="list-style-type: decimal">
<li>Normalization statistics must be calculated only with train data to
avoid <em>data leakage</em><a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a></li>
<li>Variable scales may change substantially between temporal
windows</li>
<li>NUTS requires all parameters to operate on <span class="math inline">\(\mathcal{O}(1)\)</span> scales for efficiency</li>
</ol>
</div>
<div id="interpretation-of-negative-metrics" class="section level3">
<h3><strong>Interpretation of Negative Metrics</strong></h3>
<p>It is possible to obtain <span class="math inline">\(R^2 &lt;
0\)</span> in out-of-sample evaluation when the model predicts worse
than the sample mean. This does not indicate calculation error but
genuine poor predictive performance. Similarly, RMSE ratios &gt; 1
indicate that the complete model worsens predictions relative to the
baseline.</p>
</div>
<div id="potential-extensions" class="section level3">
<h3><strong>Potential Extensions</strong></h3>
<p>The current framework admits several natural extensions:</p>
<ul>
<li><strong>Heteroscedasticity</strong>: Model <span class="math inline">\(\sigma_t\)</span> as a function of time or
covariates</li>
<li><strong>Mixture of experts</strong>: Multiple AR components with
variable weights</li>
<li><strong>Non-linearity</strong>: Splines or basis functions via
<code>s()</code> in <code>brms</code></li>
<li><strong>Random effects</strong>: Hierarchical structure for multiple
series</li>
<li><strong>Multi-step forecasting</strong>: Extend beyond
one-step-ahead</li>
</ul>
</div>
</div>
<div id="methodological-conclusions" class="section level2">
<h2><strong>Methodological Conclusions</strong></h2>
<p>The Bayesian GLM methodology with AR(1) offers a robust and
principled framework for evaluating predictive relationships between
economic variables. Unlike the ECM-MARS approach that requires
sequential validation of multiple assumptions (stationarity,
cointegration, adjustment speed), the Bayesian model integrates all
uncertainty into a unified framework.</p>
<p>Results suggest that the most robust relationships (support &gt;
0.70) typically correspond to pairs where economic theory predicts
strong causality, while relationships with moderate support (0.60-0.70)
may reflect indirect transmission channels or dependence on the
prevailing economic regime.</p>
<p>Dual evaluation via ELPD and RMSE ensures that selected models not
only fit well in terms of squared error but also adequately capture the
probabilistic structure of the data. This double requirement effectively
filters spurious relationships that might appear promising under a
single criterion.</p>
<hr />
</div>
</div>
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p>NUTS (No-U-Turn Sampler) is an adaptive extension of the
Hamiltonian Monte Carlo algorithm that automatically adjusts trajectory
lengths to maximize sampling efficiency without requiring manual tuning
of the number of leapfrog steps.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Weakly informative priors provide mild regularization
without imposing strong a priori beliefs. In the limit of abundant data,
their influence vanishes, recovering estimates similar to maximum
likelihood.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>The constant size of the training set in sliding window
ensures that all folds have comparable statistical power, avoiding bias
toward late folds that occurs with expanding windows.<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>ELPD (Expected Log Predictive Density) is the
fundamental predictive evaluation criterion in Bayesian statistics,
generalized by WAIC and LOO-CV. It measures the model’s ability to
assign high probability to new observations.<a href="#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>The NUTS algorithm eliminates the need to manually tune
the number of steps in HMC through a stopping criterion based on
“U-turns” in parameter space, where the trajectory begins to double back
toward its origin.<a href="#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>Data leakage occurs when information from the test set
contaminates the training process, artificially inflating performance
metrics. Scaling with global statistics would constitute a subtle but
pernicious form of leakage.<a href="#fnref6" class="footnote-back">↩︎</a></p></li>
</ol>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>

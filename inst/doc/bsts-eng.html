<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="José Mauricio Gómez Julián" />


<title>Methodological Details of Bayesian State-Space Models with Variable Selection</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>



<style type="text/css">
code {
white-space: pre;
}
.sourceCode {
overflow: visible;
}
</style>
<style type="text/css" data-origin="pandoc">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
{ counter-reset: source-line 0; }
pre.numberSource code > span
{ position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
{ content: counter(source-line);
position: relative; left: -1em; text-align: right; vertical-align: baseline;
border: none; display: inline-block;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }
div.sourceCode
{ }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } 
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.at { color: #7d9029; } 
code span.bn { color: #40a070; } 
code span.bu { color: #008000; } 
code span.cf { color: #007020; font-weight: bold; } 
code span.ch { color: #4070a0; } 
code span.cn { color: #880000; } 
code span.co { color: #60a0b0; font-style: italic; } 
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.do { color: #ba2121; font-style: italic; } 
code span.dt { color: #902000; } 
code span.dv { color: #40a070; } 
code span.er { color: #ff0000; font-weight: bold; } 
code span.ex { } 
code span.fl { color: #40a070; } 
code span.fu { color: #06287e; } 
code span.im { color: #008000; font-weight: bold; } 
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.kw { color: #007020; font-weight: bold; } 
code span.op { color: #666666; } 
code span.ot { color: #007020; } 
code span.pp { color: #bc7a00; } 
code span.sc { color: #4070a0; } 
code span.ss { color: #bb6688; } 
code span.st { color: #4070a0; } 
code span.va { color: #19177c; } 
code span.vs { color: #4070a0; } 
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } 
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>




<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore"><strong>Methodological Details of Bayesian
State-Space Models with Variable Selection</strong></h1>
<h4 class="author">José Mauricio Gómez Julián</h4>
<h4 class="date">octubre 2025</h4>


<div id="TOC">
<ul>
<li><a href="#state-space-model-framework-for-time-series" id="toc-state-space-model-framework-for-time-series"><strong>State-Space
Model Framework for Time Series</strong></a>
<ul>
<li><a href="#general-state-space-representation" id="toc-general-state-space-representation"><strong>General State-Space
Representation</strong></a>
<ul>
<li><a href="#mathematical-formulation" id="toc-mathematical-formulation"><strong>Mathematical
Formulation</strong></a></li>
<li><a href="#advantages-of-the-state-space-framework" id="toc-advantages-of-the-state-space-framework"><strong>Advantages of
the State-Space Framework</strong></a></li>
</ul></li>
<li><a href="#implemented-structural-components" id="toc-implemented-structural-components"><strong>Implemented
Structural Components</strong></a>
<ul>
<li><a href="#local-level-ll" id="toc-local-level-ll"><strong>Local
Level (LL)</strong></a></li>
<li><a href="#local-linear-trend-llt" id="toc-local-linear-trend-llt"><strong>Local Linear Trend
(LLT)</strong></a></li>
<li><a href="#seasonal-component-optional" id="toc-seasonal-component-optional"><strong>Seasonal Component
(Optional)</strong></a></li>
</ul></li>
<li><a href="#bayesian-variable-selection-with-spike-and-slab" id="toc-bayesian-variable-selection-with-spike-and-slab"><strong>Bayesian
Variable Selection with Spike-and-Slab</strong></a>
<ul>
<li><a href="#economic-motivation" id="toc-economic-motivation"><strong>Economic
Motivation</strong></a></li>
<li><a href="#spike-and-slab-prior" id="toc-spike-and-slab-prior"><strong>Spike-and-Slab
Prior</strong></a></li>
<li><a href="#hyperparameter-calibration" id="toc-hyperparameter-calibration"><strong>Hyperparameter
Calibration</strong></a></li>
</ul></li>
<li><a href="#bayesian-inference-via-mcmc" id="toc-bayesian-inference-via-mcmc"><strong>Bayesian Inference via
MCMC</strong></a>
<ul>
<li><a href="#sampling-algorithm" id="toc-sampling-algorithm"><strong>Sampling Algorithm</strong></a></li>
<li><a href="#mcmc-configuration" id="toc-mcmc-configuration"><strong>MCMC Configuration</strong></a></li>
<li><a href="#convergence-diagnostics" id="toc-convergence-diagnostics"><strong>Convergence
Diagnostics</strong></a></li>
</ul></li>
<li><a href="#temporal-validation-with-05" id="toc-temporal-validation-with-05"><strong>Temporal Validation with
0</strong></a>
<ul>
<li><a href="#validation-scheme" id="toc-validation-scheme"><strong>Validation Scheme</strong></a></li>
<li><a href="#procedure-per-fold" id="toc-procedure-per-fold"><strong>Procedure per Fold</strong></a></li>
</ul></li>
<li><a href="#predictive-evaluation-metrics" id="toc-predictive-evaluation-metrics"><strong>Predictive Evaluation
Metrics</strong></a>
<ul>
<li><a href="#expected-log-predictive-density-elpd" id="toc-expected-log-predictive-density-elpd"><strong>Expected Log
Predictive Density (ELPD)</strong></a></li>
<li><a href="#point-error-metrics" id="toc-point-error-metrics"><strong>Point Error
Metrics</strong></a></li>
<li><a href="#calibration-metrics" id="toc-calibration-metrics"><strong>Calibration
Metrics</strong></a></li>
</ul></li>
<li><a href="#optimal-structure-selection" id="toc-optimal-structure-selection"><strong>Optimal Structure
Selection</strong></a>
<ul>
<li><a href="#structure-grid" id="toc-structure-grid"><strong>Structure
Grid</strong></a></li>
<li><a href="#selection-criterion" id="toc-selection-criterion"><strong>Selection
Criterion</strong></a></li>
</ul></li>
<li><a href="#victory-and-stability-criterion" id="toc-victory-and-stability-criterion"><strong>Victory and Stability
Criterion</strong></a>
<ul>
<li><a href="#victory-per-fold" id="toc-victory-per-fold"><strong>Victory per Fold</strong></a></li>
<li><a href="#stability-metrics" id="toc-stability-metrics"><strong>Stability Metrics</strong></a></li>
</ul></li>
<li><a href="#computational-implementation-and-optimizations" id="toc-computational-implementation-and-optimizations"><strong>Computational
Implementation and Optimizations</strong></a>
<ul>
<li><a href="#processing-architecture" id="toc-processing-architecture"><strong>Processing
Architecture</strong></a></li>
<li><a href="#special-case-management" id="toc-special-case-management"><strong>Special Case
Management</strong></a></li>
<li><a href="#output-data-structure" id="toc-output-data-structure"><strong>Output Data
Structure</strong></a></li>
</ul></li>
<li><a href="#comparative-analysis-with-previous-methodologies" id="toc-comparative-analysis-with-previous-methodologies"><strong>Comparative
Analysis with Previous Methodologies</strong></a>
<ul>
<li><a href="#comparative-approach-table" id="toc-comparative-approach-table"><strong>Comparative Approach
Table</strong></a></li>
<li><a href="#distinctive-advantages-of-bsts" id="toc-distinctive-advantages-of-bsts"><strong>Distinctive Advantages
of BSTS</strong></a></li>
<li><a href="#relative-limitations" id="toc-relative-limitations"><strong>Relative
Limitations</strong></a></li>
</ul></li>
<li><a href="#extensions-and-future-developments" id="toc-extensions-and-future-developments"><strong>Extensions and
Future Developments</strong></a>
<ul>
<li><a href="#immediate-improvements" id="toc-immediate-improvements"><strong>Immediate
Improvements</strong></a></li>
<li><a href="#methodological-extensions" id="toc-methodological-extensions"><strong>Methodological
Extensions</strong></a></li>
</ul></li>
<li><a href="#complete-pipeline-pseudocode" id="toc-complete-pipeline-pseudocode"><strong>Complete Pipeline
Pseudocode</strong></a></li>
<li><a href="#methodological-conclusions" id="toc-methodological-conclusions"><strong>Methodological
Conclusions</strong></a></li>
</ul></li>
</ul>
</div>

<div id="state-space-model-framework-for-time-series" class="section level1">
<h1><strong>State-Space Model Framework for Time Series</strong></h1>
<p>State-space models constitute a unified framework for time series
analysis that explicitly separates the underlying signal from
observational noise. This methodology implements full Bayesian inference
on structural time series components (Bayesian Structural Time Series -
BSTS), combining the flexibility of 0<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> with automatic variable selection through
0<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>
priors. The approach allows uncertainty quantification at all model
levels while maintaining economic interpretability of the
components.</p>
<div id="general-state-space-representation" class="section level2">
<h2><strong>General State-Space Representation</strong></h2>
<div id="mathematical-formulation" class="section level3">
<h3><strong>Mathematical Formulation</strong></h3>
<p>A state-space model is defined by two fundamental equations:</p>
<p><strong>Observation Equation:</strong> <span class="math display">\[y_t = Z_t&#39; \alpha_t + \beta&#39; x_t +
\epsilon_t, \quad \epsilon_t \sim N(0, \sigma^2_{\epsilon})\]</span></p>
<p><strong>State (Transition) Equation:</strong> <span class="math display">\[\alpha_{t+1} = T_t \alpha_t + R_t \eta_t, \quad
\eta_t \sim N(0, Q_t)\]</span></p>
<p>where: - <span class="math inline">\(y_t\)</span> is the observation
at time <span class="math inline">\(t\)</span> - <span class="math inline">\(\alpha_t\)</span> is the latent state vector of
dimension <span class="math inline">\(m\)</span> - <span class="math inline">\(x_t\)</span> is the vector of exogenous regressors
of dimension <span class="math inline">\(k\)</span> - <span class="math inline">\(\beta\)</span> are the regression coefficients -
<span class="math inline">\(Z_t\)</span> connects the state to
observations - <span class="math inline">\(T_t\)</span> is the state
transition matrix - <span class="math inline">\(R_t\)</span> and <span class="math inline">\(Q_t\)</span> define the variance structure of the
state process - <span class="math inline">\(\epsilon_t\)</span> is the
observation error</p>
</div>
<div id="advantages-of-the-state-space-framework" class="section level3">
<h3><strong>Advantages of the State-Space Framework</strong></h3>
<ol style="list-style-type: decimal">
<li><strong>Structural decomposition</strong>: Separates trend,
seasonality, and irregular components</li>
<li><strong>Natural handling of missing data</strong>: The Kalman filter
automatically interpolates</li>
<li><strong>Complete uncertainty</strong>: Posterior distributions for
states and forecasts</li>
<li><strong>Modular flexibility</strong>: Components can be added as
needed</li>
</ol>
</div>
</div>
<div id="implemented-structural-components" class="section level2">
<h2><strong>Implemented Structural Components</strong></h2>
<div id="local-level-ll" class="section level3">
<h3><strong>Local Level (LL)</strong></h3>
<p>The simplest model includes only a stochastic level:</p>
<p><span class="math display">\[\begin{aligned}
y_t &amp;= \mu_t + \beta&#39; x_t + \epsilon_t \\
\mu_{t+1} &amp;= \mu_t + \eta_{\mu,t}
\end{aligned}\]</span></p>
<p>where <span class="math inline">\(\eta_{\mu,t} \sim N(0,
\sigma^2_\mu)\)</span> represents innovations in the level. This model
is appropriate for series with varying level but no systematic
trend.</p>
</div>
<div id="local-linear-trend-llt" class="section level3">
<h3><strong>Local Linear Trend (LLT)</strong></h3>
<p>Extends the LL model by adding a stochastic trend:</p>
<p><span class="math display">\[\begin{aligned}
y_t &amp;= \mu_t + \beta&#39; x_t + \epsilon_t \\
\mu_{t+1} &amp;= \mu_t + \delta_t + \eta_{\mu,t} \\
\delta_{t+1} &amp;= \delta_t + \eta_{\delta,t}
\end{aligned}\]</span></p>
<p>where: - <span class="math inline">\(\mu_t\)</span> is the level at
time <span class="math inline">\(t\)</span> - <span class="math inline">\(\delta_t\)</span> is the slope (rate of change) at
time <span class="math inline">\(t\)</span> - <span class="math inline">\(\eta_{\mu,t} \sim N(0, \sigma^2_\mu)\)</span> are
level innovations - <span class="math inline">\(\eta_{\delta,t} \sim
N(0, \sigma^2_\delta)\)</span> are slope innovations</p>
</div>
<div id="seasonal-component-optional" class="section level3">
<h3><strong>Seasonal Component (Optional)</strong></h3>
<p>For series with seasonal patterns:</p>
<p><span class="math display">\[\gamma_{t+1} = -\sum_{s=1}^{S-1}
\gamma_{t-s+1} + \eta_{\gamma,t}\]</span></p>
<p>where <span class="math inline">\(S\)</span> is the seasonal period
(e.g., 12 for monthly data) and <span class="math inline">\(\eta_{\gamma,t} \sim N(0,
\sigma^2_\gamma)\)</span>.</p>
</div>
</div>
<div id="bayesian-variable-selection-with-spike-and-slab" class="section level2">
<h2><strong>Bayesian Variable Selection with
Spike-and-Slab</strong></h2>
<div id="economic-motivation" class="section level3">
<h3><strong>Economic Motivation</strong></h3>
<p>In high-dimensional contexts with multiple potential lags, variable
selection is crucial for: - Avoiding overfitting - Maintaining
interpretability - Identifying genuine predictors</p>
</div>
<div id="spike-and-slab-prior" class="section level3">
<h3><strong>Spike-and-Slab Prior</strong></h3>
<p>For each coefficient <span class="math inline">\(\beta_j\)</span> we
define a hierarchical prior:</p>
<p><span class="math display">\[\beta_j | \gamma_j \sim \begin{cases}
N(0, \tau^2 v_j) &amp; \text{if } \gamma_j = 1 \text{ (slab)} \\
\delta_0 &amp; \text{if } \gamma_j = 0 \text{ (spike)}
\end{cases}\]</span></p>
<p>where: - <span class="math inline">\(\gamma_j \in \{0,1\}\)</span> is
the inclusion indicator - <span class="math inline">\(\tau^2
v_j\)</span> is the slab variance (non-zero component) - <span class="math inline">\(\delta_0\)</span> is a point mass at zero</p>
<p>The prior on the indicators is:</p>
<p><span class="math display">\[\gamma_j \sim
\text{Bernoulli}(\pi_j)\]</span></p>
<p>with <span class="math inline">\(\pi_j\)</span> 0<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> according to expected
model size.</p>
</div>
<div id="hyperparameter-calibration" class="section level3">
<h3><strong>Hyperparameter Calibration</strong></h3>
<p>Hyperparameters are configured via:</p>
<ol style="list-style-type: decimal">
<li><strong>Expected model size</strong>: <span class="math inline">\(E[\sum \gamma_j] = \max(1, \min(5, k))\)</span>
where <span class="math inline">\(k\)</span> is the number of
predictors</li>
<li><strong>Prior information weight</strong>: <span class="math inline">\(w = 0.01\)</span> (weakly informative prior)</li>
<li><strong>Diagonal shrinkage</strong>: <span class="math inline">\(\kappa = 0.5\)</span> for moderate
regularization</li>
</ol>
<p>This configuration balances flexibility with parsimony, allowing data
to dominate inference while maintaining smooth regularization.</p>
</div>
</div>
<div id="bayesian-inference-via-mcmc" class="section level2">
<h2><strong>Bayesian Inference via MCMC</strong></h2>
<div id="sampling-algorithm" class="section level3">
<h3><strong>Sampling Algorithm</strong></h3>
<p>Inference is performed through a hybrid Gibbs sampling scheme:</p>
<ol style="list-style-type: decimal">
<li><strong>Latent states</strong> (<span class="math inline">\(\alpha_{1:T}\)</span>): 0<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> (FFBS)</li>
<li><strong>Variances</strong> (<span class="math inline">\(\sigma^2_\epsilon, \sigma^2_\mu,
\sigma^2_\delta\)</span>): Gibbs with conjugate inverse-gamma
priors</li>
<li><strong>Coefficients and indicators</strong> (<span class="math inline">\(\beta, \gamma\)</span>): 0<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> (SSVS)</li>
</ol>
</div>
<div id="mcmc-configuration" class="section level3">
<h3><strong>MCMC Configuration</strong></h3>
<ul>
<li><strong>Total iterations</strong>: 2000</li>
<li><strong>Burn-in</strong>: 500 (25%)</li>
<li><strong>Thinning</strong>: Not applied (all post-burn samples)</li>
<li><strong>Chains</strong>: 1 (parallelization at pair level)</li>
<li><strong>Seed</strong>: Fixed for reproducibility</li>
</ul>
</div>
<div id="convergence-diagnostics" class="section level3">
<h3><strong>Convergence Diagnostics</strong></h3>
<p>While not explicitly reported for computational efficiency, standard
diagnostics include: - Visual inspection of traces - Chain
autocorrelation - Effective sample size for key parameters - Stable
inclusion probabilities for <span class="math inline">\(\gamma_j\)</span></p>
</div>
</div>
<div id="temporal-validation-with-05" class="section level2">
<h2><strong>Temporal Validation with 0<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a></strong></h2>
<div id="validation-scheme" class="section level3">
<h3><strong>Validation Scheme</strong></h3>
<p>We implement Leave-Future-Out (LFO) with rolling origin:</p>
<ol style="list-style-type: decimal">
<li><strong>Initial set</strong>: 80% of data or minimum 30
observations</li>
<li><strong>Forecast horizon</strong>: <span class="math inline">\(h =
6\)</span> months</li>
<li><strong>Step between origins</strong>: 6 months</li>
<li><strong>Window type</strong>: Expanding (accumulates history)</li>
</ol>
<p>This more conservative scheme (6-month horizon and step vs 12 in
ECM-MARS) allows greater temporal resolution in stability
evaluation.</p>
</div>
<div id="procedure-per-fold" class="section level3">
<h3><strong>Procedure per Fold</strong></h3>
<p>For each fold <span class="math inline">\(f\)</span>:</p>
<ol style="list-style-type: decimal">
<li><strong>Data preparation</strong>:
<ul>
<li>Split into train/test according to split</li>
<li>Scale regressors per fold using train statistics</li>
</ul></li>
<li><strong>Model fitting</strong>:
<ul>
<li><strong>Base</strong>: Only structural components without
regressors</li>
<li><strong>Full</strong>: Structural components + regressors with
spike-and-slab</li>
</ul></li>
<li><strong>Probabilistic prediction</strong>:
<ul>
<li>Generate complete posterior predictive distribution</li>
<li>Extract mean, standard deviation, and quantiles</li>
</ul></li>
<li><strong>Evaluation</strong>:
<ul>
<li>Calculate ELPD, point metrics, and calibration</li>
</ul></li>
</ol>
</div>
</div>
<div id="predictive-evaluation-metrics" class="section level2">
<h2><strong>Predictive Evaluation Metrics</strong></h2>
<div id="expected-log-predictive-density-elpd" class="section level3">
<h3><strong>Expected Log Predictive Density (ELPD)</strong></h3>
<p>For each future observation <span class="math inline">\(y_t^*\)</span>:</p>
<p><span class="math display">\[\text{ELPD}_t = \log \int p(y_t^* |
\theta) p(\theta | \mathcal{D}_{\text{train}}) d\theta\]</span></p>
<p>Approximated via MCMC samples as:</p>
<p><span class="math display">\[\widehat{\text{ELPD}}_t = \log \left(
\frac{1}{S} \sum_{s=1}^S p(y_t^* | \theta^{(s)}) \right)\]</span></p>
<p>where we assume normality: <span class="math inline">\(p(y_t^* |
\theta^{(s)}) = N(y_t^*; \mu_t^{(s)}, \sigma_t^{(s)})\)</span>.</p>
</div>
<div id="point-error-metrics" class="section level3">
<h3><strong>Point Error Metrics</strong></h3>
<p>Using the posterior mean as point prediction:</p>
<ul>
<li><strong>RMSE</strong>: <span class="math inline">\(\sqrt{\frac{1}{h}
\sum_{t=1}^h (y_t - \bar{y}_t)^2}\)</span></li>
<li><strong>MAE</strong>: <span class="math inline">\(\frac{1}{h}
\sum_{t=1}^h |y_t - \bar{y}_t|\)</span></li>
</ul>
<p>where <span class="math inline">\(\bar{y}_t = \frac{1}{S}
\sum_{s=1}^S y_t^{(s)}\)</span> is the posterior mean.</p>
</div>
<div id="calibration-metrics" class="section level3">
<h3><strong>Calibration Metrics</strong></h3>
<p>We evaluate the quality of quantified uncertainty:</p>
<ul>
<li><strong>80% Coverage</strong>: Proportion of observations within 80%
credible interval</li>
<li><strong>95% Coverage</strong>: Proportion of observations within 95%
credible interval</li>
<li><strong>PIT (Probability Integral Transform)</strong>: <span class="math inline">\(u_t = F_{Y_t}(y_t)\)</span> where <span class="math inline">\(F_{Y_t}\)</span> is the predictive CDF</li>
</ul>
<p>If the model is well-calibrated, PIT values follow a uniform
distribution on [0,1].</p>
</div>
</div>
<div id="optimal-structure-selection" class="section level2">
<h2><strong>Optimal Structure Selection</strong></h2>
<div id="structure-grid" class="section level3">
<h3><strong>Structure Grid</strong></h3>
<p>We systematically evaluate:</p>
<table>
<colgroup>
<col width="13%" />
<col width="13%" />
<col width="13%" />
<col width="25%" />
<col width="33%" />
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Level</th>
<th>Trend</th>
<th>Seasonality</th>
<th>Free Parameters</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>LL</td>
<td>Stochastic</td>
<td>No</td>
<td>Optional</td>
<td><span class="math inline">\(\sigma^2_\mu,
\sigma^2_\epsilon\)</span></td>
</tr>
<tr class="even">
<td>LLT</td>
<td>Stochastic</td>
<td>Stochastic</td>
<td>Optional</td>
<td><span class="math inline">\(\sigma^2_\mu, \sigma^2_\delta,
\sigma^2_\epsilon\)</span></td>
</tr>
</tbody>
</table>
</div>
<div id="selection-criterion" class="section level3">
<h3><strong>Selection Criterion</strong></h3>
<p>The optimal structure is selected through:</p>
<ol style="list-style-type: decimal">
<li><strong>Primary</strong>: Highest average <span class="math inline">\(\Delta\text{ELPD}\)</span> across folds</li>
<li><strong>Secondary</strong>: Highest support (proportion of winning
folds)</li>
<li><strong>Tertiary</strong>: Highest <span class="math inline">\(\Delta\text{RMSE}\)</span> (error reduction)</li>
</ol>
<p>This hierarchy prioritizes probabilistic predictive capacity over
point fit.</p>
</div>
</div>
<div id="victory-and-stability-criterion" class="section level2">
<h2><strong>Victory and Stability Criterion</strong></h2>
<div id="victory-per-fold" class="section level3">
<h3><strong>Victory per Fold</strong></h3>
<p>A full model “wins” in fold <span class="math inline">\(f\)</span>
if:</p>
<p><span class="math display">\[\begin{cases}
\Delta\text{ELPD}_f &gt; 0 &amp; \text{(better predictive density)} \\
\Delta\text{RMSE}_f &gt; 0 &amp; \text{(lower error, i.e.,
RMSE}_{\text{base}} - \text{RMSE}_{\text{full}} &gt; 0\text{)}
\end{cases}\]</span></p>
<p>Note: Unlike GLM-AR(1), here <span class="math inline">\(\Delta\text{RMSE} = \text{RMSE}_{\text{base}} -
\text{RMSE}_{\text{full}}\)</span>, so positive values indicate
improvement.</p>
</div>
<div id="stability-metrics" class="section level3">
<h3><strong>Stability Metrics</strong></h3>
<ul>
<li><strong>Support</strong>: <span class="math inline">\(\frac{\text{wins}}{\text{folds}}\)</span> =
proportion of victorious folds</li>
<li><strong>Strict threshold</strong>: support <span class="math inline">\(\geq 0.70\)</span> with minimum 5 folds</li>
<li><strong>Moderate threshold</strong>: support <span class="math inline">\(\geq 0.60\)</span> with minimum 5 folds</li>
</ul>
</div>
</div>
<div id="computational-implementation-and-optimizations" class="section level2">
<h2><strong>Computational Implementation and Optimizations</strong></h2>
<div id="processing-architecture" class="section level3">
<h3><strong>Processing Architecture</strong></h3>
<ol style="list-style-type: decimal">
<li><strong>Parallelization by pairs</strong>: The 84 pairs are
processed sequentially</li>
<li><strong>MCMC efficiency</strong>: Single chain per model (speed vs
diagnostics trade-off)</li>
<li><strong>Matrix caching</strong>: Reuse of decompositions in Kalman
filter</li>
</ol>
</div>
<div id="special-case-management" class="section level3">
<h3><strong>Special Case Management</strong></h3>
<p>The code includes robust handling of:</p>
<ul>
<li><strong>Zero variance</strong>: Skip fold if <span class="math inline">\(\text{sd}(y) \approx 0\)</span></li>
<li><strong>Convergence failures</strong>: Try-catch with informative
messages</li>
<li><strong>Singular matrices</strong>: Automatic regularization in
Kalman filter</li>
<li><strong>Degenerate predictions</strong>: Fallback to quantile-based
intervals</li>
</ul>
</div>
<div id="output-data-structure" class="section level3">
<h3><strong>Output Data Structure</strong></h3>
<p>Each pair generates:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="fu">list</span>(</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>  <span class="at">best_summary =</span> <span class="fu">tibble</span>(     <span class="co"># Best model summary</span></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>    spec, folds, wins, support, </span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a>    dELPD_mean, dRMSE_mean, ...</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a>  ),</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a>  <span class="at">best_results =</span> <span class="fu">tibble</span>(     <span class="co"># Details per fold</span></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a>    fold, n_train, n_test,</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a>    ELPD_base, ELPD_full, dELPD,</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a>    RMSE_base, RMSE_full, dRMSE,</span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a>    cover80, cover95, pit, ...</span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a>  ),</span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a>  <span class="at">all_summaries =</span> <span class="fu">tibble</span>()   <span class="co"># All tested structures</span></span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a>)</span></code></pre></div>
</div>
</div>
<div id="comparative-analysis-with-previous-methodologies" class="section level2">
<h2><strong>Comparative Analysis with Previous
Methodologies</strong></h2>
<div id="comparative-approach-table" class="section level3">
<h3><strong>Comparative Approach Table</strong></h3>
<table>
<colgroup>
<col width="22%" />
<col width="28%" />
<col width="31%" />
<col width="17%" />
</colgroup>
<thead>
<tr class="header">
<th>Aspect</th>
<th>ECM-MARS</th>
<th>GLM-AR(1)</th>
<th>BSTS</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Paradigm</strong></td>
<td>Hybrid frequentist</td>
<td>Parametric Bayesian</td>
<td>Structural Bayesian</td>
</tr>
<tr class="even">
<td><strong>Prerequisites</strong></td>
<td>I(1), cointegration</td>
<td>None</td>
<td>None</td>
</tr>
<tr class="odd">
<td><strong>Non-linearity</strong></td>
<td>MARS (splines)</td>
<td>No</td>
<td>No (but flexible components)</td>
</tr>
<tr class="even">
<td><strong>Variable selection</strong></td>
<td>Manual (pre-tests)</td>
<td>No</td>
<td>Automatic (spike-slab)</td>
</tr>
<tr class="odd">
<td><strong>Temporal components</strong></td>
<td>ECM only</td>
<td>Global AR(1)</td>
<td>Level, trend, seasonality</td>
</tr>
<tr class="even">
<td><strong>Uncertainty</strong></td>
<td>Implicit bootstrap</td>
<td>Complete posterior</td>
<td>Complete posterior + states</td>
</tr>
<tr class="odd">
<td><strong>Interpretability</strong></td>
<td>High (economic)</td>
<td>Medium</td>
<td>High (decomposition)</td>
</tr>
<tr class="even">
<td><strong>Computational cost</strong></td>
<td>Medium</td>
<td>High</td>
<td>Very high</td>
</tr>
</tbody>
</table>
</div>
<div id="distinctive-advantages-of-bsts" class="section level3">
<h3><strong>Distinctive Advantages of BSTS</strong></h3>
<ol style="list-style-type: decimal">
<li><p><strong>Interpretable decomposition</strong>: Separates signal
from noise with economically meaningful components</p></li>
<li><p><strong>Automatic selection</strong>: Spike-and-slab identifies
relevant predictors without pre-specification</p></li>
<li><p><strong>Structural uncertainty handling</strong>: Quantifies
uncertainty in unobservable components</p></li>
<li><p><strong>Robustness to specification</strong>: Does not require
assumptions about integration order or cointegration</p></li>
<li><p><strong>Interval forecasts</strong>: Natural and well-calibrated
prediction intervals</p></li>
</ol>
</div>
<div id="relative-limitations" class="section level3">
<h3><strong>Relative Limitations</strong></h3>
<ol style="list-style-type: decimal">
<li><p><strong>High computational cost</strong>: FFBS + SSVS is
intensive, especially with many regressors</p></li>
<li><p><strong>Linearity in predictors</strong>: Does not capture
non-linear interactions like MARS</p></li>
<li><p><strong>Complex diagnostics</strong>: Convergence harder to
verify than OLS</p></li>
<li><p><strong>Sensitivity to priors in small samples</strong>:
Especially for component variances</p></li>
</ol>
</div>
</div>
<div id="extensions-and-future-developments" class="section level2">
<h2><strong>Extensions and Future Developments</strong></h2>
<div id="immediate-improvements" class="section level3">
<h3><strong>Immediate Improvements</strong></h3>
<ol style="list-style-type: decimal">
<li><strong>Additional components</strong>:
<ul>
<li><strong>Calendar effects</strong>: Business days, moving
holidays</li>
<li><strong>Level shifts</strong>: Automatic detection of structural
breaks</li>
<li><strong>Dynamic regressors</strong>: Time-varying coefficients</li>
</ul></li>
<li><strong>Informative priors</strong>:
<ul>
<li>Incorporate economic information in inclusion priors</li>
<li>Hierarchical priors for groups of related variables</li>
</ul></li>
<li><strong>Enhanced validation</strong>:
<ul>
<li>Stochastic cross-validation (Pareto smoothed importance
sampling)</li>
<li>Backtesting with multiple horizons</li>
</ul></li>
</ol>
</div>
<div id="methodological-extensions" class="section level3">
<h3><strong>Methodological Extensions</strong></h3>
<ol style="list-style-type: decimal">
<li><strong>Non-linearity</strong>:
<ul>
<li>Gaussian processes for non-linear components</li>
<li>Bayesian neural networks for interaction capture</li>
</ul></li>
<li><strong>Hierarchical models</strong>:
<ul>
<li>Partial pooling between related pairs</li>
<li>Shared latent factors</li>
</ul></li>
<li><strong>Causality</strong>:
<ul>
<li>Intervention models with causal inference</li>
<li>Dynamic causal graphs</li>
</ul></li>
</ol>
</div>
</div>
<div id="complete-pipeline-pseudocode" class="section level2">
<h2><strong>Complete Pipeline Pseudocode</strong></h2>
<pre><code>INPUT: DataFrame with time series, variable specification
OUTPUT: Ranking of pairs by predictive capacity and stability

# PREPARATION
1. LOAD data and clean names
2. IDENTIFY 6 circulation variables, 7 production
3. GENERATE 84 pairs (2 × 6 × 7)

# PROCESSING PER PAIR
FOR each pair (Y, X):
  
  4. BUILD matrix with Y, X, lags(X, 1:6)
  5. REMOVE observations with NA
  
  # STRUCTURE TUNING
  FOR each structure in {LL, LLT}:
    
    # LEAVE-FUTURE-OUT
    6. GENERATE LFO splits (init=80%, h=6, step=6)
    
    FOR each fold:
      7. SPLIT train/test
      8. SCALE regressors with train stats
      
      # MODELS
      9. FIT base_model:
         - state_spec = structure without regressors
         - MCMC with 2000 iter, 500 burn-in
      
      10. FIT full_model:
          - state_spec = structure
          - prior = spike_slab(X_lags, expected_size=5)
          - MCMC with 2000 iter, 500 burn-in
      
      # PREDICTION
      11. GENERATE predictive distributions (h=6)
      12. EXTRACT mean, sd, quantiles
      
      # EVALUATION
      13. CALCULATE:
          - ELPD_base, ELPD_full → ΔELPD
          - RMSE_base, RMSE_full → ΔRMSE
          - Coverage 80%, 95%
          - PIT values
      
      14. DETERMINE victory: (ΔELPD &gt; 0) ∧ (ΔRMSE &gt; 0)
    
    15. SUMMARIZE structure:
        - support = wins/folds
        - metric averages
  
  16. SELECT best structure by ΔELPD
  17. SAVE pair results

# RANKING AND EXPORT
18. ORDER pairs by (support DESC, ΔELPD DESC, ΔRMSE DESC)
19. FILTER winners by thresholds (0.70, 0.60)
20. EXPORT CSVs and visualizations
21. GENERATE plots for last fold (without re-fitting)</code></pre>
</div>
<div id="methodological-conclusions" class="section level2">
<h2><strong>Methodological Conclusions</strong></h2>
<p>The BSTS methodology represents the most comprehensive and flexible
approach of the three implemented, combining the rigor of Bayesian
inference with the interpretability of structural decomposition. Unlike
ECM-MARS which requires prior validation of strict econometric
assumptions, and GLM-AR(1) which models only global serial dependence,
BSTS decomposes the series into interpretable components while
automatically selecting relevant predictors.</p>
<p>Empirical results suggest that BSTS identifies robust predictive
relationships with high precision, albeit at substantially higher
computational cost. The ability to quantify uncertainty at all
levels—from structural components to predictions—makes it particularly
valuable for applications where risk assessment is critical.</p>
<p>The implementation with a 6-month horizon (vs 12 in other
methodologies) provides more granular evaluation of temporal stability,
revealing that many relationships appearing stable at long horizons show
variability at shorter scales. This suggests the importance of
considering multiple horizons in predictive model evaluation.</p>
<p>The BSTS framework is especially appropriate when: - Component
interpretation is a priority - There is uncertainty about which
predictors to include - Well-calibrated prediction intervals are
required - Data exhibit multiple sources of variation (trend,
seasonality, regressors)</p>
<p>The combination of automatic variable selection with structural
modeling positions BSTS as a bridge between classical econometric
methods and modern machine learning techniques, maintaining
interpretability while embracing the inherent complexity in economic
data.</p>
<hr />
</div>
</div>
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p>Structural decomposition separates systematic variation
(trend, seasonality) from random variation, facilitating economic
interpretation and improving forecasts by modeling each component
appropriately.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>The spike-and-slab prior combines a continuous
distribution (slab) for active coefficients with a point mass at zero
(spike) for inactive coefficients, enabling exact variable selection.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>Calibration of predictive probabilities is crucial for
decision-making under uncertainty. A well-calibrated model assigns
probabilities that reflect long-run empirical frequencies.<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>Forward Filtering Backward Sampling (FFBS) is the
standard algorithm for sampling states in linear Gaussian models,
combining forward Kalman filtering with backward sampling conditioned on
complete data.<a href="#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>Stochastic Search Variable Selection (SSVS) is an MCMC
method for exploring model space, alternating between updating
coefficients given the model and updating the model given
coefficients.<a href="#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>The validation scheme with h=6 and step=6 avoids overlap
between test sets, ensuring independence in evaluation while maximizing
data usage.<a href="#fnref6" class="footnote-back">↩︎</a></p></li>
</ol>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
